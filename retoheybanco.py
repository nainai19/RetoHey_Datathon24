# -*- coding: utf-8 -*-
"""retoheybanco.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lCkDoPddcsk7nnOw6P8eUZ9OGWqdTdPR
"""

#Importar las librerías
import pandas as pd
import numpy as np
import nltk   #Documentación: https://www.nltk.org/
nltk.download('punkt')
# import nltk.corpus   #Documentación: https://www.nltk.org/api/nltk.corpus.html?highlight=corpus#module-nltk.corpus

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

carpeta_path = '/content/drive/MyDrive/Datathon2024/'
archivo = 'bdd_categ.csv'
ruta_completa = os.path.join(carpeta_path, archivo)

# Verificar si el archivo existe
if os.path.exists(ruta_completa):
    # Cargar el archivo en un DataFrame
    df = pd.read_csv(ruta_completa, encoding='ISO-8859-1')  # Especificar la codificación
    print("DataFrame cargado correctamente:")
    print(df.head())  # Imprimir las primeras filas del DataFrame
else:
    print("El archivo bd no existe en la carpeta especificada.")

import os
import pandas as pd

carpeta_path = '/content/drive/MyDrive/Datathon2024/'
archivo = 'bdd_categ.csv'
ruta_completa = os.path.join(carpeta_path, archivo)

#Verificar si el archivo existe
if os.path.exists(ruta_completa):
    # Cargar el archivo en un DataFrame
    try:
        df = pd.read_csv(ruta_completa, encoding='latin1')  # Intenta con diferentes codificaciones
        print("DataFrame cargado correctamente:")
        print(df.info())  # Imprimir información del DataFrame
    except UnicodeDecodeError as e:
        print("Error de decodificación:", e)
else:
    print("El archivo bd no existe en la carpeta especificada.")

#split_date = df['date'].str.split(' ', expand=True)
#df['fecha'] = split_date[0]
#df['hora'] = split_date[1] if len(split_date.columns) > 1 else ''
#df['hora_comentario'] = df.apply(lambda row: row['time'] if row['time'] != '' else row['hora'], axis=1)
#df.drop(['time', 'hora', 'date'], axis=1, inplace=True)

#df['fecha'] = pd.to_datetime(df['fecha'], format='%d-%m-%Y')
#df['hora_comentario'] = pd.to_datetime(df['hora_comentario'], format='%H:%M:%S').dt.time

df['comentario '] = df['comentario '].astype(str)
df['red social '] = df['red social '].astype(str)
df['categoria'] = df['categoria'].astype('Int64')
df['sub categoria'] = df['sub categoria'].astype('Int64')

print(df.head())

df.isna().sum()

from nltk.tokenize import word_tokenize
df['token'] = df['comentario '].apply(word_tokenize)
print(df['token'])

from nltk.probability import FreqDist
all_tokens = [token for sublist in df['comentario '].apply(word_tokenize) for token in sublist]
fdist = FreqDist(all_tokens)
fdist1 = fdist.most_common(10)
fdist1

from nltk.probability import FreqDist
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

!pip install nltk
import nltk
nltk.download('stopwords')

signos = {',', '.', '!', '?', ':', '%', '(', ')', '@', '...'}

stop_words = set(stopwords.words('spanish'))

palabras_a_omitir = stop_words | signos


all_tokens = [token.lower() for sublist in df['comentario '].apply(word_tokenize) for token in sublist]

filtered_tokens = [token for token in all_tokens if token not in palabras_a_omitir]

fdist = FreqDist(filtered_tokens)

fdist1 = fdist.most_common(20)

print(fdist1)

from nltk.probability import FreqDist
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

all_tokens = [token.lower() for sublist in df['comentario '].apply(word_tokenize) for token in sublist]

# Filtrar los tokens para omitir las palabras especificadas y convertir a minúsculas
filtered_tokens = [token for token in all_tokens if token.lower() not in palabras_a_omitir]

# Calcular la frecuencia de distribución de los tokens filtrados
fdist = FreqDist(filtered_tokens)

print(fdist)

# Gráfico de distribución de frecuencias usando matplotlib
import matplotlib.pyplot as plt
fdist.plot(20,cumulative=False)
plt.show()

#!pip install transformers

import pandas as pd
from transformers import BertModel, BertTokenizer
import torch

# Cargar tokenizer y modelo
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Función para convertir texto a embedding
def text_to_embedding(text):
    # Tokenizar el texto y convertir a tensores de PyTorch
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Obtener los outputs del modelo
    with torch.no_grad():
        outputs = model(**inputs)

    # Obtener los embeddings del último estado oculto
    # Aquí, puedes elegir obtener el embedding del token CLS o hacer un promedio de todos los embeddings
    embeddings = outputs.last_hidden_state[:, 0, :].squeeze()
    return embeddings.numpy()

# Aplicar la función al DataFrame
df['embeddings'] = df['comentario '].apply(text_to_embedding)

!pip install torch

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import random

# Función para configurar la semilla
def set_seed(seed_value=42):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Aplicar la configuración de semilla
set_seed()

# Implementación mejorada del modelo de red neuronal
class EnhancedNN_v2(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(EnhancedNN_v2, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(128, output_dim)
        self.relu = nn.LeakyReLU(negative_slope=0.01)

    def forward(self, x):
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        x = self.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x

# Supongamos que 'df' es tu DataFrame y tiene columnas 'embeddings' y 'category'
input_dim = 768  # Asumiendo que este es el tamaño de tus embeddings BERT
output_dim = 7   # Asumiendo que tienes 7 categorías diferentes

model = EnhancedNN_v2(input_dim, output_dim)  # Crear instancia del modelo con las dimensiones correctas

# Define loss y optimizador con decay de peso para regularización L2
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

print("Modelo creado con éxito y sin errores de tipo.")

"""Completa"""

carpeta_path = '/content/drive/MyDrive/Datathon2024/'
archivo = 'bdd_completa_1.csv'
ruta_completa = os.path.join(carpeta_path, archivo)

# Verificar si el archivo existe
if os.path.exists(ruta_completa):
    # Cargar el archivo en un DataFrame
    df = pd.read_csv(ruta_completa, encoding='ISO-8859-1')  # Especificar la codificación
    print("DataFrame cargado correctamente:")
    print(df.head())  # Imprimir las primeras filas del DataFrame
else:
    print("El archivo bd no existe en la carpeta especificada.")

split_date = df['date'].str.split(' ', expand=True)
df['fecha'] = split_date[0]
df['hora'] = split_date[1] if len(split_date.columns) > 1 else ''
df['hora_comentario'] = df.apply(lambda row: row['time'] if row['time'] != '' else row['hora'], axis=1)
df.drop(['time', 'hora', 'date'], axis=1, inplace=True)

#df['fecha'] = pd.to_datetime(df['fecha'], format='%d-%m-%Y')
df['hora_comentario'] = pd.to_datetime(df['hora_comentario'], format='%H:%M:%S').dt.time

df['comentario '] = df['comentario '].astype(str)
df['red social '] = df['red social '].astype(str)

print(df.head())

from nltk.tokenize import word_tokenize
df['token'] = df['comentario '].apply(word_tokenize)
print(df['token'])

all_tokens = [token for sublist in df['comentario '].apply(word_tokenize) for token in sublist]
fdist = FreqDist(all_tokens)
fdist1 = fdist.most_common(10)
fdist1

signos = {',', '.', '!', '?', ':', '%', '(', ')', '@', '...', '*'}

stop_words = set(stopwords.words('spanish'))

palabras_a_omitir = stop_words | signos


all_tokens = [token.lower() for sublist in df['comentario '].apply(word_tokenize) for token in sublist]

filtered_tokens = [token for token in all_tokens if token not in palabras_a_omitir]

fdist = FreqDist(filtered_tokens)

fdist1 = fdist.most_common(20)

print(fdist1)

all_tokens = [token.lower() for sublist in df['comentario '].apply(word_tokenize) for token in sublist]

# Filtrar los tokens para omitir las palabras especificadas y convertir a minúsculas
filtered_tokens = [token for token in all_tokens if token.lower() not in palabras_a_omitir]

# Calcular la frecuencia de distribución de los tokens filtrados
fdist = FreqDist(filtered_tokens)

print(fdist)

import matplotlib.pyplot as plt
fdist.plot(20,cumulative=False)
plt.show()

from transformers import BertModel, BertTokenizer
import torch

# Cargar el tokenizer y el modelo BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model_bert = BertModel.from_pretrained('bert-base-uncased')

# Configurar el modelo BERT en modo de evaluación
model_bert.eval()

def get_bert_embeddings(comments):
    embeddings = []
    with torch.no_grad():
        for comment in comments:
            # Tokenizar el comentario y convertirlo a tensor de PyTorch
            inputs = tokenizer(comment, return_tensors="pt", padding=True, truncation=True, max_length=512)
            # Obtener la salida del modelo BERT
            outputs = model_bert(**inputs)
            # Extraer el embedding del token CLS (puedes modificar esto según tus necesidades)
            cls_embedding = outputs.last_hidden_state[:, 0, :]
            embeddings.append(cls_embedding.squeeze().numpy())
    return embeddings

# Aplicar la función a la columna "comentarios"
df['embeddings'] = get_bert_embeddings(df['comentario '].tolist())

df.head()

import torch
import numpy as np

# Asegurar que el modelo está en modo evaluación
model.eval()

# Convertir embeddings de numpy a tensores de PyTorch
embeddings_tensor = torch.tensor(np.stack(df['embeddings'].values), dtype=torch.float)

# Realizar predicciones con el modelo
with torch.no_grad():
    outputs = model(embeddings_tensor)
    _, predicted_categories = torch.max(outputs, 1)

# Convertir las categorías predichas a numpy si es necesario
predicted_categories = predicted_categories.numpy()

# Añadir las predicciones al DataFrame
df['categoria_numerica'] = predicted_categories + 1  # Ajustando índice si es necesario

# Mapeo de categorías numéricas a nombres de categoría
category_map = {
    1: "Agradecimiento",
    2: "Queja",
    3: "Solicitud de información",
    4: "Recomendación",
    5: "Elogio",
    6: "Sarcástico",
    7: "Otro"
}

df['categoria'] = df['categoria_numerica'].map(category_map)

# Mostrar las primeras filas para verificar
print(df[['comentario ', 'categoria_numerica', 'categoria']].head())

df.to_excel('bdd_final.xlsx', index=False, engine='openpyxl')